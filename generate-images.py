import os
import json
from huggingface_hub import InferenceClient
from PIL import Image
from dotenv import load_dotenv

# === CONFIG ===
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN", "your_huggingface_token_here")  # Reemplaza con tu token de Hugging Face
INPUT_JSON = "stories/1.json"
OUTPUT_DIR = "imagenes_hf"

os.makedirs(OUTPUT_DIR, exist_ok=True)
client = InferenceClient(token=HF_TOKEN)

# === Leer JSON ===
with open(INPUT_JSON, encoding="utf-8") as f:
    data = json.load(f)
imagenes = data.get("imagenes", [])

# === Generar im√°genes ===
for idx, img in enumerate(imagenes, 1):
    prompt = img["descripcion"]
    print(f"\nüñºÔ∏è Generando imagen {idx:03} ‚Üí {prompt}")
    try:
        image = client.text_to_image(prompt)  # selecciona autom√°ticamente un modelo v√°lido
    except Exception as e:
        print("‚ùå Error:", e)
        continue

    output_path = os.path.join(OUTPUT_DIR, f"{idx:03}.png")
    image.save(output_path)
    print(f"‚úÖ Imagen guardada en {output_path}")

print("\n‚úÖ ¬°Todas las im√°genes generadas!")
